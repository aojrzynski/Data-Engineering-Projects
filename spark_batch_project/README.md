# Spark Batch Processing Project

## ğŸ“Š Overview
This project demonstrates batch data processing using **Apache Spark** to analyze and visualize transactional data. The workflow includes data ingestion, cleaning, transformation, analysis, and visualization of financial transactions to uncover spending patterns and detect potential fraudulent activities.

## ğŸ›  Tools and Technologies
- **Apache Spark:** Distributed data processing.
- **PySpark:** Python API for Spark.
- **Pandas:** Data manipulation.
- **Matplotlib & Seaborn:** Data visualization.
- **Docker:** Containerized Spark environment.
- **Parquet:** Efficient data storage format.

## ğŸš€ Project Workflow
1. **Data Ingestion:** Load raw transactional data in CSV format and convert it to Parquet for optimized storage.
2. **Data Cleaning & Transformation:** Handle missing values, remove unnecessary columns, and create derived columns.
3. **Data Analysis:** Aggregate and summarize data by merchant category and country.
4. **Data Visualization:** Generate bar plots to visualize transaction trends and high-value transactions.

## ğŸ“‚ Project Structure
```
â”œâ”€â”€ data                  # Raw datasets (.csv files)
â”œâ”€â”€ notebooks             # Jupyter Notebooks (guides and analysis)
â”œâ”€â”€ outputs               # Processed data and visualizations
â”œâ”€â”€ scripts               # Python scripts for each step
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â”œâ”€â”€ clean_transform.py
â”‚   â”œâ”€â”€ analyze_data.py
â”‚   â”œâ”€â”€ visualize_outputs.py
â”‚   â”œâ”€â”€ sample_dataset.py
â”‚   â”œâ”€â”€ test_dataset.py
â”‚   â””â”€â”€ verify_cleaned_data.py
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md             # Project documentation
```

## âš™ï¸ Installation and Setup

### 1. Install Prerequisites
- **Python 3.8+**: Install from [python.org](https://www.python.org/)
- **Docker Desktop**: Install from [docker.com](https://www.docker.com/)

### 2. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 3. Pull Apache Spark Docker Image
```bash
docker pull bitnami/spark
```

### 4. Run the Docker Container
```bash
docker run -it -v /path/to/your/project:/app bitnami/spark /bin/bash
```

---

## ğŸ“ˆ Running the Project

### Step 1: Data Ingestion
```bash
python scripts/ingest_data.py
```

### Step 2: Data Cleaning and Transformation
```bash
python scripts/clean_transform.py
```

### Step 3: Data Analysis
```bash
python scripts/analyze_data.py
```

### Step 4: Data Visualization
```bash
python scripts/visualize_outputs.py
```

---

## ğŸ“¥ Dataset Access
Due to GitHub's file size restrictions, the raw dataset is not included.

### Option 1: Download Dataset
- Download the dataset [here](https://www.kaggle.com/datasets/ismetsemedov/transactions) and place it in the `data/` folder as:
  ```
  data/synthetic_fraud_data.csv
  ```

### Option 2: Generate Sample Data, or use provided sample file
- Run the script to create a sample dataset:
  ```bash
  python scripts/sample_dataset.py
  ```

---

## ğŸ“Š Outputs and Results
After running the project, check the **outputs/** folder:

- **Processed Data:**
  - `raw_dataset.parquet` â†’ Ingested data
  - `cleaned_dataset.parquet` â†’ Cleaned data
- **Analysis Results:**
  - `merchant_summary.csv`
  - `high_value_summary.csv`
- **Visualizations:**
  - `total_amount_by_category.png`
  - `high_value_amount_by_country.png`

---

## ğŸ” Optional Scripts
- **`sample_dataset.py`** â†’ Creates a smaller dataset for testing.
- **`test_dataset.py`** â†’ Verifies dataset loading.
- **`verify_cleaned_data.py`** â†’ Checks cleaned data.

---

## ğŸ“„ License
This project is licensed under the MIT License.
