{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "540ac596",
   "metadata": {},
   "source": [
    "# Spark Batch Processing Project Guide\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ae47b",
   "metadata": {},
   "source": [
    "Welcome to the **Spark Batch Processing Project**! This project demonstrates how to perform batch data processing using **Apache Spark**. The goal is to process a transactional dataset through data ingestion, cleaning, transformation, analysis, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19cafa6",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Project Overview](#Project-Overview)\n",
    "2. [Dataset Information](#Dataset-Information)\n",
    "3. [Installation Guide](#Installation-Guide)\n",
    "4. [Running the Project](#Running-the-Project)\n",
    "5. [Outputs and Results](#Outputs-and-Results)\n",
    "6. [Optional Scripts](#Optional-Scripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca1378",
   "metadata": {},
   "source": [
    "## 1. Project Overview\n",
    "---\n",
    "\n",
    "The project is structured into the following components:\n",
    "\n",
    "- **Data Ingestion:** Reads raw data and converts it into an optimized format (Parquet).\n",
    "- **Data Cleaning and Transformation:** Handles missing values, drops irrelevant columns, and adds derived columns.\n",
    "- **Data Analysis:** Aggregates and summarizes data for insights.\n",
    "- **Visualization:** Generates visual representations of the analysis.\n",
    "\n",
    "### Project Folder Structure:\n",
    "```\n",
    "├── data                  # Raw datasets (.csv files)\n",
    "├── notebooks             # Jupyter Notebooks (this guide)\n",
    "├── outputs               # Processed data and visualizations\n",
    "├── scripts               # Python scripts for each project step\n",
    "│   ├── ingest_data.py\n",
    "│   ├── clean_transform.py\n",
    "│   ├── analyze_data.py\n",
    "│   ├── visualize_outputs.py\n",
    "│   ├── sample_dataset.py\n",
    "│   ├── test_dataset.py\n",
    "│   └── verify_cleaned_data.py\n",
    "├── requirements.txt      # Python dependencies\n",
    "└── README.md             # Project documentation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc701d",
   "metadata": {},
   "source": [
    "## 2. Dataset Information\n",
    "---\n",
    "\n",
    "This project uses a **synthetic transactional dataset** designed to simulate real-world financial transactions. The dataset contains information on purchases made by customers across various merchant categories. It is structured to help identify spending patterns and potential fraudulent activity.\n",
    "\n",
    "### **Dataset Location:**\n",
    "- Located in the `data/` folder:\n",
    "  - `synthetic_fraud_data.csv`\n",
    "  - `sample_fraud_data.csv`\n",
    "\n",
    "### **Dataset Features:**\n",
    "- `transaction_id`: Unique identifier for each transaction.\n",
    "- `customer_id`: Unique identifier for each customer.\n",
    "- `card_number`: Encrypted credit/debit card number.\n",
    "- `timestamp`: Date and time of the transaction.\n",
    "- `merchant_category`: Category of the merchant (e.g., Grocery, Travel).\n",
    "- `merchant_type`: Type of merchant (e.g., physical store, online).\n",
    "- `amount`: Transaction amount.\n",
    "- `currency`: Currency type (e.g., USD, GBP).\n",
    "- `country`: Country where the transaction occurred.\n",
    "- `city`: City of the merchant.\n",
    "- `card_present`: Boolean indicating if the card was physically present.\n",
    "- `high_risk_merchant`: Boolean indicating if the merchant is high-risk.\n",
    "- `distance_from_home`: Distance between the customer’s home and the merchant.\n",
    "- `is_fraud`: Boolean flag indicating if the transaction is fraudulent.\n",
    "\n",
    "### **Purpose:**\n",
    "- To simulate **fraud detection** by analyzing transactional patterns.\n",
    "- To demonstrate how **batch processing** can be used to clean, transform, and analyze large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a14670",
   "metadata": {},
   "source": [
    "## 3. Installation Guide\n",
    "---\n",
    "\n",
    "### 1. Install Docker\n",
    "- Download and install **Docker Desktop** from [Docker's Official Website](https://www.docker.com/products/docker-desktop/).\n",
    "- After installation, start Docker and ensure it is running.\n",
    "- Verify installation:\n",
    "```bash\n",
    "docker --version\n",
    "```\n",
    "\n",
    "### 2. Install Python and Dependencies\n",
    "- Install **Python 3.8+** from [Python's Official Website](https://www.python.org/downloads/).\n",
    "- Install Python packages:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 3. Pull the Apache Spark Docker Image\n",
    "- Open your terminal and run:\n",
    "```bash\n",
    "docker pull bitnami/spark\n",
    "```\n",
    "\n",
    "### 4. Run the Docker Container\n",
    "```bash\n",
    "docker run -it -v /path/to/your/project:/app bitnami/spark /bin/bash\n",
    "```\n",
    "*Replace `/path/to/your/project` with your actual project folder path.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386d524",
   "metadata": {},
   "source": [
    "## 4. Running the Project\n",
    "---\n",
    "\n",
    "### Step 1: Data Ingestion\n",
    "```bash\n",
    "python scripts/ingest_data.py\n",
    "```\n",
    "\n",
    "### Step 2: Data Cleaning and Transformation\n",
    "```bash\n",
    "python scripts/clean_transform.py\n",
    "```\n",
    "\n",
    "### Step 3: Data Analysis\n",
    "```bash\n",
    "python scripts/analyze_data.py\n",
    "```\n",
    "\n",
    "### Step 4: Data Visualization\n",
    "```bash\n",
    "python scripts/visualize_outputs.py\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3222c12",
   "metadata": {},
   "source": [
    "## 5. Outputs and Results\n",
    "---\n",
    "\n",
    "After running the project, check the **outputs/** folder for the following:\n",
    "\n",
    "- **Processed Data:**\n",
    "  - `raw_dataset.parquet` → Ingested data\n",
    "  - `cleaned_dataset.parquet` → Cleaned and transformed data\n",
    "- **Analysis Results:**\n",
    "  - `merchant_summary.csv` → Transaction summary by merchant category\n",
    "  - `high_value_summary.csv` → High-value transactions summary by country\n",
    "- **Visualizations:**\n",
    "  - `total_amount_by_category.png`\n",
    "  - `high_value_amount_by_country.png`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f541ce",
   "metadata": {},
   "source": [
    "## 6. Optional Scripts\n",
    "---\n",
    "\n",
    "The following scripts are optional but useful for testing and development:\n",
    "\n",
    "- **`sample_dataset.py`**: Creates a smaller sample of the raw dataset for quicker testing.\n",
    "- **`test_dataset.py`**: Verifies that the dataset can be properly loaded into Spark (useful for testing schema and data integrity).\n",
    "- **`verify_cleaned_data.py`**: Loads and inspects the cleaned dataset to ensure that data transformations were correctly applied.\n",
    "\n",
    "Run these scripts as needed for development or troubleshooting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
